<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Our Paper is Accepted by NeurIPS 2025 as Spotlight | Ran Cheng </title> <meta name="author" content="Ran Cheng"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/emi-icon.png?be09e2749e3cf5872cc58cb183baed2b"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ranchengcn.github.io/news/announcement_4/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ran</span> Cheng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/news/">news <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/services/">services </a> </li> <li class="nav-item "> <a class="nav-link" href="/awards/">awards </a> </li> <li class="nav-item "> <a class="nav-link" href="/contact/">contact </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Our Paper is Accepted by NeurIPS 2025 as Spotlight</h1> <p class="post-meta"> Created in September 20, 2025 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </p> </header> <article class="post-content"> <div id="markdown-content"> <hr> <h3 id="diversity-aware-policy-optimization-for-large-language-model-reasoning">Diversity-Aware Policy Optimization for Large Language Model Reasoning</h3> <p><strong>Authors:</strong> Jian Yao; Ran Cheng; Xingyu Wu; Jibin Wu; Kay Chen Tan<br> <strong>Conference:</strong> <em>The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025, Spotlight)</em><br> <strong>Paper:</strong> <a href="https://arxiv.org/abs/2505.23433" rel="external nofollow noopener" target="_blank">[2505.23433] Diversity-Aware Policy Optimization for Large Language Model Reasoning</a><br> <strong>Code:</strong> <a href="https://github.com/nigelyaoj/R1_zero_Div" rel="external nofollow noopener" target="_blank">GitHub - nigelyaoj/R1_zero_Div</a></p> <hr> <h3 id="-a-new-look-at-reasoning-in-the-era-of-reinforcement-tuned-llms">üåü A New Look at Reasoning in the Era of Reinforcement-Tuned LLMs</h3> <p>Large Language Models (LLMs) have achieved remarkable reasoning performance in recent years, largely fueled by Reinforcement Learning (RL)‚Äìbased fine-tuning techniques. Since <strong>DeepSeek-R1</strong> introduced the <em>Group Relative Policy Optimization (GRPO)</em> framework, numerous studies have sought to refine reward design and training efficiency‚Äîpushing the limits of mathematical and logical reasoning.</p> <p>Yet, one critical factor that has long proven vital in classical RL‚Äîthe <strong>diversity of policies</strong>‚Äîhas been largely overlooked. When RL is applied to enhance LLM reasoning, does policy diversity still matter?<br> This question lies at the heart of <em>Diversity-Aware Policy Optimization for LLM Reasoning</em>, the first systematic study revealing the importance of diversity in RL-tuned LLMs and introducing a new approach‚Äî<strong>R1-zero-Div</strong>‚Äîthat explicitly incorporates diversity into the RL fine-tuning process.</p> <hr> <h3 id="-understanding-the-background-why-diversity-matters">üîç Understanding the Background: Why Diversity Matters</h3> <p>Traditional RL research has consistently shown that <strong>policy diversity</strong> promotes exploration, prevents premature convergence, and improves generalization. Analogously, in LLM reasoning tasks, ‚Äúdiverse solutions‚Äù may represent different chains of thought or logical pathways‚Äîessentially, varied <em>ways of thinking</em>.</p> <p>Building on this insight, the authors systematically investigated whether diversity could similarly affect the learning potential of LLMs under RL fine-tuning. Their findings demonstrate that diversity is not merely aesthetic‚Äîit is <em>predictive</em> of further learning gains.</p> <hr> <h3 id="-model-diversity-analysis">üß™ Model Diversity Analysis</h3> <p>To test this hypothesis, the team conducted a comprehensive study involving <strong>12 representative LLMs</strong>, ranging from base to RL-tuned models, evaluated on the <strong>MATH</strong> dataset.<br> They examined three key indicators:</p> <ol> <li> <p><strong>Solution Diversity (Div-Equ):</strong> structural variability in problem-solving formulas.</p> </li> <li> <p><strong>Potential@k (k = 16):</strong> the estimated improvement capacity after RL fine-tuning.</p> </li> <li> <p><strong>Pass@1 Accuracy:</strong> correctness under single sampling.</p> </li> </ol> <p>The results revealed a striking pattern:</p> <blockquote> <p>For strong reasoners (Pass@1 &gt; 0.4), diversity and improvement potential are <strong>strongly correlated</strong>‚Äîmodels that can generate more varied correct solutions are more likely to improve further through RL.<br> In essence, diversity is a catalyst for sustained reasoning growth.</p> </blockquote> <hr> <h3 id="Ô∏è-the-proposed-method-r1-zero-div">‚öôÔ∏è The Proposed Method: R1-zero-Div</h3> <p>Motivated by these findings, the authors extended the <strong>R1-zero</strong> framework to encourage diversity explicitly.<br> Their method introduces a <strong>token-level entropy regularization</strong> term to measure the richness of decision-making during generation‚Äîwithout biasing toward longer sequences. To ensure learning quality, this regularization is applied <strong>only to correct samples</strong>, ensuring a balanced optimization between <em>quality</em> and <em>diversity</em>.</p> <p>Formally, R1-zero-Div augments the GRPO loss with a controlled diversity term, creating a training objective that rewards not only correctness but also the <em>variety</em> of valid reasoning paths.<br> The approach is lightweight, requires <strong>no extra supervision</strong>, and integrates seamlessly into existing RL pipelines.</p> <hr> <h3 id="-experimental-highlights">üìà Experimental Highlights</h3> <p>Using <strong>Qwen2.5-Math</strong> as the base model, R1-zero-Div was evaluated across four reasoning benchmarks: <strong>GSM8K</strong>, <strong>MATH500</strong>, <strong>OlympiadBench</strong>, and <strong>College Math</strong>.</p> <table> <thead> <tr> <th>Dataset</th> <th>Baseline (R1-zero)</th> <th>R1-zero-Div</th> <th>Œî Pass@1</th> </tr> </thead> <tbody> <tr> <td>GSM8K</td> <td>88.7</td> <td><strong>91.7</strong></td> <td>+3.0</td> </tr> <tr> <td>MATH500</td> <td>75.6</td> <td><strong>78.2</strong></td> <td>+2.6</td> </tr> <tr> <td>OlympiadBench</td> <td>‚Äì</td> <td>‚Üë</td> <td>+</td> </tr> <tr> <td>College Math</td> <td>‚Äì</td> <td>‚Üë</td> <td>+</td> </tr> </tbody> </table> <p>Beyond accuracy, R1-zero-Div produced <strong>structurally richer and more varied correct solutions</strong>, as verified through Div-Equ, n-gram diversity, and BLEU metrics.<br> Ablation studies further confirmed that applying diversity regularization only on correct samples yields optimal gains, while excessive regularization may degrade performance. Even smaller-scale models benefited consistently‚Äîproving the method‚Äôs <strong>scalability and universality</strong>.</p> <hr> <h3 id="-key-takeaways">üß© Key Takeaways</h3> <ul> <li> <p><strong>First systematic analysis</strong> of diversity‚Äôs role in RL-tuned LLM reasoning.</p> </li> <li> <p><strong>Novel diversity-aware optimization</strong> seamlessly integrated into R1-zero.</p> </li> <li> <p><strong>Improved reasoning accuracy</strong> (+3.5% Pass@1 on average) and <strong>solution diversity</strong>.</p> </li> <li> <p><strong>Lightweight, generalizable, and training-efficient</strong>‚Äîno extra data required.</p> </li> </ul> <p>Together, these findings reveal that <strong>diversity is not a by-product but a driver</strong> of advanced reasoning in LLMs.</p> <hr> <h3 id="-looking-ahead">üöÄ Looking Ahead</h3> <p>This work opens new directions for the next generation of reasoning-enhanced LLMs.<br> Future research will explore <strong>semantic-level diversity</strong>, moving beyond statistical entropy toward capturing distinct reasoning strategies and thought patterns, and validating the approach across larger models and cross-domain tasks.</p> <p>By bridging traditional RL insights with modern LLM reasoning, <em>Diversity-Aware Policy Optimization</em> marks a significant step toward <strong>smarter, more explorative, and more human-like AI reasoning systems</strong>.</p> <hr> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Ran Cheng. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>