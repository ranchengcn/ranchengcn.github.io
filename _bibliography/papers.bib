---
---

@article{evox,
  abstract = {Inspired by natural evolutionary processes, Evolutionary Computation (EC) has established itself as a cornerstone of Artificial Intelligence. Recently, with the surge in data-intensive applications and large-scale complex systems, the demand for scalable EC solutions has grown significantly. However, most existing EC infrastructures fall short of catering to the heightened demands of large-scale problem solving. While the advent of some pioneering GPU-accelerated EC libraries is a step forward, they also grapple with some limitations, particularly in terms of flexibility and architectural robustness. In response, we introduce EvoX: a computing framework tailored for automated, distributed, and heterogeneous execution of EC algorithms. At the core of EvoX lies a unique programming model to streamline the development of parallelizable EC algorithms, complemented by a computation model specifically optimized for distributed GPU acceleration. Building upon this foundation, we have crafted an extensive library comprising a wide spectrum of 50+ EC algorithms for both single-and multi-objective optimization. Furthermore, the library offers comprehensive support for a diverse set of benchmark problems, ranging from dozens of numerical test functions to hundreds of reinforcement learning tasks. Through extensive experiments across a range of problem scenarios and hardware configurations, EvoX demonstrates robust system and model performances. EvoX is open-source and accessible at: https://github.com/EMI-Group/EvoX.}
  title = {{EvoX}: A Distributed {GPU}-accelerated Framework for Scalable Evolutionary Computation},
  author = {Huang, Beichen and Cheng, Ran and Li, Zhuozhao and Jin, Yaochu and Tan, Kay Chen},
  journal = {IEEE Transactions on Evolutionary Computation},
  year = 2024,
  doi = {10.1109/TEVC.2024.3388550}
}

@article{evoxbench,
  abstract = {The ongoing advancements in network architecture design have led to remarkable achievements in deep learning across various challenging computer vision tasks. Meanwhile, the development of neural architecture search (NAS) has provided promising approaches to automating the design of network architectures for lower prediction error. Recently, the emerging application scenarios of deep learning (e.g., autonomous driving) have raised higher demands for network architectures considering multiple design criteria: number of parameters/weights, number of floating-point operations, inference latency, among others. From an optimization point of view, the NAS tasks involving multiple design criteria are intrinsically multiobjective optimization problems; hence, it is reasonable to adopt evolutionary multiobjective optimization (EMO) algorithms for tackling them. Nonetheless, there is still a clear gap confining the related research along this pathway: on the one hand, there is a lack of a general problem formulation of NAS tasks from an optimization point of view; on the other hand, there are challenges in conducting benchmark assessments of EMO algorithms on NAS tasks. To bridge the gap: 1) we formulate NAS tasks into general multiobjective optimization problems and analyze the complex characteristics from an optimization point of view; 2) we present an end-to-end pipeline, dubbed EvoXBench , to generate benchmark test problems for EMO algorithms to run efficientlyâ€”without the requirement of GPUs or Pytorch/Tensorflow; and 3) we instantiate two test suites comprehensively covering two datasets, seven search spaces, and three hardware devices, involving up to eight objectives. Based on the above, we validate the proposed test suites using six representative EMO algorithms and provide some empirical analyses. The code of EvoXBench is available at https://github.com/EMI-Group/EvoXBench.}
  title={Neural architecture search as multiobjective optimization benchmarks: Problem formulation and performance assessment},
  author={Lu, Zhichao and Cheng, Ran and Jin, Yaochu and Tan, Kay Chen and Deb, Kalyanmoy},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={28},
  number={2},
  pages={323--337},
  year={2023},
  publisher={IEEE}
}

@article{rvea,
 abstract = {In evolutionary multiobjective optimization, maintaining a good balance between convergence and diversity is particularly crucial to the performance of the evolutionary algorithms (EAs). In addition, it becomes increasingly important to incorporate user preferences because it will be less likely to achieve a representative subset of the Pareto-optimal solutions using a limited population size as the number of objectives increases. This paper proposes a reference vector-guided EA for many-objective optimization. The reference vectors can be used not only to decompose the original multiobjective optimization problem into a number of single-objective subproblems, but also to elucidate user preferences to target a preferred subset of the whole Pareto front (PF). In the proposed algorithm, a scalarization approach, termed angle-penalized distance, is adopted to balance convergence and diversity of the solutions in the high-dimensional objective space. An adaptation strategy is proposed to dynamically adjust the distribution of the reference vectors according to the scales of the objective functions. Our experimental results on a variety of benchmark test problems show that the proposed algorithm is highly competitive in comparison with five state-of-the-art EAs for many-objective optimization. In addition, we show that reference vectors are effective and cost-efficient for preference articulation, which is particularly desirable for many-objective optimization. Furthermore, a reference vector regeneration strategy is proposed for handling irregular PFs. Finally, the proposed algorithm is extended for solving constrained many-objective optimization problems.},
 annote = {Conference Name: IEEE Transactions on Evolutionary Computation},
 author = {Cheng, Ran and Jin, Yaochu and Olhofer, Markus and Sendhoff, Bernhard},
 doi = {10.1109/TEVC.2016.2519378},
 issn = {1941-0026},
 journal = {IEEE Transactions on Evolutionary Computation},
 keywords = {Evolutionary computation, Pareto optimization, Sociology, Convergence, Linear programming, evolutionary multiobjective optimization, Angle-penalized distance (APD), convergence, diversity, many-objective optimization, preference articulation, reference vector},
 number = {5},
 pages = {773--791},
 title = {A Reference Vector Guided Evolutionary Algorithm for Many-Objective Optimization},
 volume = {20},
 year = {2016}
}

@article{cso,
 abstract = {In this paper, a novel competitive swarm optimizer (CSO) for large scale optimization is proposed. The algorithm is fundamentally inspired by the particle swarm optimization but is conceptually very different. In the proposed CSO, neither the personal best position of each particle nor the global best position (or neighborhood best positions) is involved in updating the particles. Instead, a pairwise competition mechanism is introduced, where the particle that loses the competition will update its position by learning from the winner. To understand the search behavior of the proposed CSO, a theoretical proof of convergence is provided, together with empirical analysis of its exploration and exploitation abilities showing that the proposed CSO achieves a good balance between exploration and exploitation. Despite its algorithmic simplicity, our empirical results demonstrate that the proposed CSO exhibits a better overall performance than five state-of-the-art metaheuristic algorithms on a set of widely used large scale optimization problems and is able to effectively solve problems of dimensionality up to 5000.},
 annote = {Conference Name: IEEE Transactions on Cybernetics},
 author = {Cheng, Ran and Jin, Yaochu},
 doi = {10.1109/TCYB.2014.2322602},
 issn = {2168-2275},
 journal = {IEEE Transactions on Cybernetics},
 keywords = {Optimization, Convergence, Particle swarm optimization, Vectors, Heuristic algorithms, Algorithm design and analysis, Competition, competitive swarm optimizer, convergence analysis, Cybernetics, large scale optimization, learning, particle swarm optimization},
 number = {2},
 pages = {191--204},
 title = {A Competitive Swarm Optimizer for Large Scale Optimization},
 volume = {45},
 year = {2015}
}



